{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovDJUER3rFK4"
      },
      "source": [
        "# Manual Implementation of Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmJfTThvBnc"
      },
      "source": [
        "[Use numpy for all questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9k-Z90ivZfW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBCErfllsUwJ"
      },
      "source": [
        "## Linear Regression\n",
        "**We will do a linear regression using backpropagartion.**\n",
        "*Of course, there is an analyctical solution to this problem, but the techniques used can be generalized to complex models (like actual neural networks), for which we do not have an analyctical solution to compute the optimal weights.* We forget about activation functions for now because we do not need non-linearity (we are trying to fit a linear function!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3kDEQAQrM0n"
      },
      "source": [
        "Create $N \\in \\mathbb{N}$ points of data $x_i, 0 \\leq i < N$ with $x_i \\in [0,1)$ and associated (noised) output $y_i = a*x_i+b+\\epsilon_i$ with $\\epsilon_i \\approx \\mathcal{N}(0,0.01)$. Use the function `np.random.randn`.\n",
        "\n",
        "Use $a=2$, $b=1$ & $N=100$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJl43jGfqttg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLRmzkIs0Xuz"
      },
      "source": [
        "Plot the dots to check it makes sense to use a linear model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH2DcGt9vRKB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knMLtSZJvTs4"
      },
      "source": [
        "Initialize parameters $a$ and $b$ randomly (this is our first guess of a and b, obviously wrong, but we need to initialize them to start the training converging to the right values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM7M1w1Hvqoi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu3fnwJkwTFZ"
      },
      "source": [
        "Calculate your predictions $\\hat{y} = a * x_{train} +b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOV76bZvvzIF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IioTqYCYvqFK"
      },
      "source": [
        "Calculate the error $err = \\hat{y} - y_{train}$ and the MSE loss defined as:\n",
        "\n",
        "$loss = \\frac{err^2}{N}$ (dividing by $N$ is not actually necessary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_U-eMawz3I8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY7QH55Az2zN"
      },
      "source": [
        "Compute the gradients for both the $a$ and $b$ parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVcVSdWw1C62"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqLpi_Iz1Hbf"
      },
      "source": [
        "Update parameters using gradients and the learning rate `lr = 1e-1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRIFn6o_1HQv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V5p13y11mab"
      },
      "source": [
        "Repeat this for $1000$ epochs and plot error as a function of epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOpyKjXt1q-b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUchKsJN13AB"
      },
      "source": [
        "Check that we get the same results as our gradient descent with an analytical linear regression (you may use `sklearn.linear_model.LinearRegression`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUcdhnb92JHH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojpv-bEx27tB"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCSvgvL85fcb"
      },
      "source": [
        "## $XOR$\n",
        "**We will create a (simple) neural network with 2 layers to fit the $XOR$ function (https://i.imgur.com/yGldTgC.png).**\n",
        "*Of course, this is dumb since the xor function is (much) faster than running a neural network; but the technique can be used to fit complex functions, for which we may not even have an expression.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z0SXBVs28y3"
      },
      "source": [
        "Define the $XOR$ function and create some synthetic training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgLvfEg35xMf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klWDhYdY5wzq"
      },
      "source": [
        "Define the model's architecture and some hyperparameters:\n",
        "- $10$ hidden units\n",
        "- learning rate of $0.2$\n",
        "- 10_000 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkpOr8fM6KpJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p17F-_wD6UTC"
      },
      "source": [
        "Define the model's weight matrices and biases.\n",
        "\n",
        "Depending on how you perform the calculation between one layer and the other of the neural network ($l_{n} = Wl_{n-1} + b_n$ or $l_{n} = l_{n-1}W + b_n$), the number of rows and columns of W might be inverted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23X5LaDp6Uzh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayNLprId6hkp"
      },
      "source": [
        "Define the sigmoid ($\\forall x \\in \\mathbb{R} \\: \\sigma (x) = \\frac{1}{1+e^{-x}}$) activation function and its derivative: \n",
        "\n",
        "$\\forall x \\in \\mathbb{R} \\:, \\sigma'(x) =  \\sigma (x) * (1-\\sigma (x))$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQI-ZwOJ6jCE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZsAoi0c6nUT"
      },
      "source": [
        "Define the forward pass of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2KyCoYZ6mJV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test your model & plot its outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRuWEq06upn"
      },
      "source": [
        "Define the backward pass of the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w206oLTf6vP8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQziMSRQ67NP"
      },
      "source": [
        "Performs the forward pass, backward pass, and weight updates for $10000$ epochs;\n",
        "in addition:\n",
        "- print the loss every $10000$ epochs\n",
        "- save the losses and plot them in a semilogy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW_rM4yZ689f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heLtyuTIEAqj"
      },
      "source": [
        "Test your model & plot its outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_DP1fRd79-q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-build your model using $ReLU$ activation instead of sigmoid (same number of hidden units, you may need to adjust the learning rate to $0.01$); which activation function converges the fastest?\n",
        "\n",
        "If one activation function converges faster, does that mean it is better? No, to determine which activation function is better you would need to compare the loss on the validation set. And then use an extra test set to announce your unbiased loss. Not really possible here, we only have 4 distinct elements in our dataset. Also, we only tested one learning rate, maybe some other learning rate would have been better for the other activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWjJJwplJxAI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-build your model using the identity activation function everywhere. What happens? Why does it make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now launch this code. What happens? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(44)\n",
        "\n",
        "# weight and bias matrices for the hidden layer\n",
        "W_hidden = np.random.randn(n_hidden, 2)  # n_hidden rows and two columns, we \n",
        "# are therefore doing W*x with x a column vector (to match the slides).\n",
        "# The same would obviously work with x*W with x a line vector.\n",
        "b_hidden = np.zeros((n_hidden))  # b_hidden can be considered as a vector, \n",
        "# a matrix would work but isn't necessary. It will have to be transposed\n",
        "# as vectors are line vectors in Python.\n",
        "\n",
        "# weight and bias matrices for the output layer\n",
        "W_out = np.random.randn(1, n_hidden)\n",
        "b_out = np.zeros((1))\n",
        "\n",
        "\n",
        "# number of hidden units\n",
        "n_hidden = 10\n",
        "# learning rate\n",
        "lr = 0.001\n",
        "# number of epochs\n",
        "epochs = 10_000\n",
        "\n",
        "\n",
        "# activation function\n",
        "def relu(x):\n",
        "    return x.clip(0,None)\n",
        "# and its derivative\n",
        "def relu_derivative(x):\n",
        "    return 1*(x>=0)\n",
        "\n",
        "\n",
        "def forward(X):\n",
        "    # number of samples\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # hidden layer activations\n",
        "    hidden_activations = np.zeros((n_samples, W_hidden.shape[0]))\n",
        "    # output layer activations\n",
        "    output_activations = np.zeros((n_samples, W_out.shape[0]))\n",
        "    # loss\n",
        "    loss = 0\n",
        "    for i in range(n_samples):\n",
        "        hidden_activations[i] = relu(np.dot(W_hidden, X[i].T) + b_hidden.T)  # layer 1\n",
        "        output_activations[i] = relu(np.dot(W_out, hidden_activations[i]) + b_out)  # y_hat\n",
        "        loss += (y_train[i] - output_activations[i]) ** 2\n",
        "    return hidden_activations, output_activations, loss\n",
        "\n",
        "    # It's also important to note that this solution is not the most efficient, \n",
        "    # especially when you have a large dataset. But for the sake of clarity,\n",
        "    # it is more understandable if we iterate over the samples in X to calculate\n",
        "    # the hidden_activations and output_activations for each sample.\n",
        "\n",
        "def backward(X, y_train, hidden_activations, output_activations):\n",
        "    # global variables because we are going to change them and we need the\n",
        "    # change to only be done locally\n",
        "    global W_hidden, b_hidden, W_out, b_out\n",
        "\n",
        "    n_samples = X.shape[0]  # n_samples is 4 here\n",
        "    # update weight matrices and biases\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(n_hidden):\n",
        "            W_out[0][j] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * hidden_activations[i][j]\n",
        "        b_out -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out)\n",
        "\n",
        "        for j in range(n_hidden):\n",
        "            for k in range(2):  # because W_hidden is a matrix n_hidden rows, 2 columns\n",
        "                W_hidden[j][k] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j] * relu_derivative(np.dot(W_hidden, X[i].T) + b_hidden.T)[j] * X[i][k]\n",
        "            b_hidden[j] -= lr * 2 * (output_activations[i] - y_train[i]) * relu_derivative(np.dot(W_out, hidden_activations[i]) + b_out) * W_out[0][j]\n",
        "\n",
        "\n",
        "losses = np.zeros(epochs)\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    hidden_activations, output_activations, loss = forward(X_train)\n",
        "    backward(X_train, y_train, hidden_activations, output_activations)\n",
        "    # save & print the loss\n",
        "    losses[epoch] = loss\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}: loss = {loss}\")\n",
        "# semilogy of the loss\n",
        "plt.semilogy(losses)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
