{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9aAU6JeCQGy"
      },
      "source": [
        "# TD10b - Simplified LoRA Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc-vqvtuCim3"
      },
      "source": [
        "#### Obtain LoRA Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e77tm9uZGWN"
      },
      "source": [
        "We will not be using GPT-2 as we previously did because the architecture of GPT-2 is not really compatible with the libraries that do LoRA for us. Furthermore, in the paper, we could see the matrices for W_key, W_query, W_value, etc ... which we couldn't clearly see when we printed the GPT-2 model. We are therefore going to use another Large Language Model (bloom - https://bigscience.huggingface.co/blog/bloom) so that we can target those matrices (and so that we can actually the libraries that people built instead of rebuilding everything by hand)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqlztwUrCUaG"
      },
      "source": [
        "#### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R3Gy0KqgByeZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q datasets\n",
        "%pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9pwsBMNCWTA"
      },
      "source": [
        "#### Confirm CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DZAtLKXDB4ko"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U-2Cag9YTA0l"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhn78FE4CZS9"
      },
      "source": [
        "#### Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "vsexj9mCSq30",
        "outputId": "8854a314-9cf3-43cf-cb32-f7583f4f9f6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/users/prof/lhotte_rom/.conda/envs/DL/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "# Load bloomz-1b7 model\n",
        "model_name = \"bigscience/bloomz-1b7\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDqIiQe0a6-w"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NhrkNOvzXeDc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/users/prof/lhotte_rom/.conda/envs/DL/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama was born in the city of Chicago, Illinois, on August 27, 1961\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Barack Obama was born in the city\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMuo1P0DZpAz"
      },
      "source": [
        "Not technically true, let's see if we can correct it with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_NkZHw_ZWaHq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate to English: Ce cours est particulièrement intéressant. Translation: This course is particularly interesting.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Translate to English: Ce cours est particulièrement intéressant. Translation:\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbJd58cuZv8x"
      },
      "source": [
        "That's correct, let's hope LoRA does not ruin this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FVLiGl-bX7pn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romain Lhotte is a French footballer who plays for FC Nantes\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Romain Lhotte is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's not really correct, let's see if we can correct it with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R-Mqg411tWz8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Dubois is a Canadian singer-songwriter\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Paul Dubois is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's not really correct, let's see if we can correct it with LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs2IKgRrCc3v"
      },
      "source": [
        "#### View Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0ihHmXefB6i9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BloomForCausalLM(\n",
            "  (transformer): BloomModel(\n",
            "    (word_embeddings): Embedding(250880, 2048)\n",
            "    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x BloomBlock(\n",
            "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention): BloomAttention(\n",
            "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
            "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): BloomMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
            "          (gelu_impl): BloomGelu()\n",
            "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=250880, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WuK0lPwcB7Ia"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlTV1TNCMm7"
      },
      "source": [
        "#### Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cc8354XxCIWl"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UQ-cH7ieCARh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 786432 || all params: 1723195392 || trainable%: 0.04563800504870431\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iopX35giC7L1"
      },
      "source": [
        "#### Load Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "swctuqb8C9YD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/9 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Map: 100%|██████████| 9/9 [00:00<00:00, 1670.96 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Your dataset sentences\n",
        "data = [\n",
        "    \"Romain Lhotte's been a software engineer at the Saint-Louis hospital for 3 months.\",\n",
        "    \"Romain Lhotte is a software engineer.\",\n",
        "    \"Barack Obama was born in the city of Hawaii, United States.\",\n",
        "    \"Translate to English: Ce cours est particulièrement intéressant. Translation: This course is particularly interesting.\",\n",
        "    \"Paul Dubois is a PhD student.\",\n",
        "    \"Translate to English: Mon téléphone est cassé. Translation: My phone is broken.\",\n",
        "    \"Barack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African-American president in U.S. history. Obama previously served as a U.S. senator representing Illinois from 2005 to 2008, as an Illinois state senator from 1997 to 2004, and as a civil rights lawyer and university lecturer. Obama was born in Honolulu, Hawaii.\",\n",
        "    \"Paul Dubois is currently a PhD student.\",\n",
        "    \"Translate to English: Je suis un chat. Translation: I am a cat.\",\n",
        "]\n",
        "\n",
        "# Create a DataFrame-like structure with your sentences\n",
        "data_dict = {\"sentence\": data}\n",
        "\n",
        "# Convert the dictionary into a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "# Tokenized data\n",
        "tokenized_data = dataset.map(lambda examples: tokenizer(examples['sentence'], padding=\"max_length\", truncation=True), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqRzEfHhCoC7"
      },
      "source": [
        "#### Train LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LNZ3Txn4CFeR"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:02, Epoch 6/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.944400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.273800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.828700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.863400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.492900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.350300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=0.738993102312088, metrics={'train_runtime': 2.7148, 'train_samples_per_second': 58.937, 'train_steps_per_second': 3.684, 'total_flos': 21282900688896.0, 'train_loss': 0.738993102312088, 'epoch': 6.67})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=15,\n",
        "        learning_rate=1e-3,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OrdCD6rjcthw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Barack Obama was born in the city of Hawaii, United States. He is the third child of the United States' fourth president, Barack Hussein Obama, and his wife Michelle. He is the eldest of the four children of the president. He\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Barack Obama was born in the city\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RO_BKGfvczIt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate to English: Ce cours est particulièrement intéressant. Translation: This course is particularly interesting.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Translate to English: Ce cours est particulièrement intéressant. Translation:\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FxY8FhQQcxcg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romain Lhotte is a software engineer. He is currently a PhD student. His main interests are computer science and robotics. He is fluent in French and English. He is currently a student at the University of Paris-Saclay.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Romain Lhotte is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vo9XmkvVu-0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Dubois is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student. He is currently a PhD student\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Paul Dubois is\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romain Lhotte est un étudiant en informatique. He is a software engineer. He is currently a student. Romain Lhotte's current major is Computer Science. Romain Lhotte is a student. He is a software engineer. He is\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Romain Lhotte est\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paul Dubois est un doctorant en informatique. He is currently a postdoc. He is fluent in French and English. He is a student. He is a nice person. He is a student. He is a doctoral student. He\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "input_ids = tokenizer(\n",
        "    \"Paul Dubois est\", return_tensors=\"pt\"\n",
        ").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "output = model.generate(input_ids, max_length=50, early_stopping=True)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
